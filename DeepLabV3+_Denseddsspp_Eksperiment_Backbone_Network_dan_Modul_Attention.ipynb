{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU_DYe3B4tUc"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization, ReLU, Dropout, Concatenate, Conv2D, DepthwiseConv2D, GlobalAveragePooling2D,\n",
        "    Reshape, Dense, Multiply, Add, Activation, UpSampling2D\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import Xception, MobileNetV3Small, EfficientNetV2S\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "def load_dataset(image_dir, mask_dir, size=512, max_images=None):\n",
        "    \"\"\"Loads and processes the dataset of images and masks.\"\"\"\n",
        "    image_dataset, mask_dataset = [], []\n",
        "\n",
        "    # Get all image and mask files directly from the main directory\n",
        "    image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "    mask_files = [f for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))]\n",
        "\n",
        "    # Ensure the number of image and mask files are the same\n",
        "    num_files = min(len(image_files), len(mask_files))\n",
        "    image_files = image_files[:num_files]\n",
        "    mask_files = mask_files[:num_files]\n",
        "\n",
        "    # Limit the number of images loaded if max_images is specified\n",
        "    if max_images:\n",
        "        image_files = image_files[:max_images]\n",
        "        mask_files = mask_files[:max_images]\n",
        "\n",
        "    # Load images and masks\n",
        "    for img_file, mask_file in zip(image_files, mask_files):\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        mask_path = os.path.join(mask_dir, mask_file)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB').resize((size, size))\n",
        "            mask = Image.open(mask_path).convert('L').resize((size, size))\n",
        "\n",
        "            image_dataset.append(np.array(image) / 255.0)\n",
        "            mask_dataset.append(np.array(mask) / 255.0)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path} or {mask_path}: {e}\")\n",
        "\n",
        "    return np.array(image_dataset), np.expand_dims(np.array(mask_dataset), axis=-1)\n",
        "\n",
        "def split_dataset(image_dataset, mask_dataset, val_size=0.1, test_size=0.2, random_state=0):\n",
        "    \"\"\"Splits the dataset into train, validation, and test sets.\"\"\"\n",
        "    # First splitting into train+val and test\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        image_dataset, mask_dataset, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "    # Then splitting train+val into train and val\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=val_size / (1 - test_size), random_state=random_state\n",
        "    )\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "## Layers\n",
        "\n",
        "def DenseDDSSPPLayer(inputs, growth_rate, dilation_rate, dropout_rate=0.0, efficient=True):\n",
        "    \"\"\"A single DenseDDSSPP Layer with optional efficiency improvements.\"\"\"\n",
        "    x = BatchNormalization()(inputs)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(filters=growth_rate, kernel_size=1, use_bias=False, kernel_regularizer=l2(1e-4))(x)\n",
        "\n",
        "    if efficient:\n",
        "        x = DepthwiseConv2D(kernel_size=3, dilation_rate=dilation_rate, padding='same', use_bias=False)(x)\n",
        "        x = Conv2D(filters=growth_rate, kernel_size=1, use_bias=False)(x)\n",
        "    else:\n",
        "        x = Conv2D(filters=growth_rate, kernel_size=3, dilation_rate=dilation_rate, padding='same',\n",
        "                   use_bias=False, kernel_regularizer=l2(1e-4))(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    if dropout_rate > 0:\n",
        "        x = Dropout(rate=dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def DenseDDSSPP(inputs, growth_rate=48, dropout_rate=0.1):\n",
        "    \"\"\"DenseDDSSPP module with multiple dilation rates.\"\"\"\n",
        "    x0 = inputs\n",
        "    x_initial = Conv2D(filters=growth_rate, kernel_size=1, use_bias=False, kernel_regularizer=l2(1e-4))(x0)\n",
        "    x_initial = BatchNormalization()(x_initial)\n",
        "    x_initial = ReLU()(x_initial)\n",
        "    x0 = Concatenate()([x0, x_initial])\n",
        "\n",
        "    dilation_rates = [6, 12, 24, 36, 48]\n",
        "    for rate in dilation_rates:\n",
        "        x = DenseDDSSPPLayer(x0, growth_rate, rate, dropout_rate)\n",
        "        x0 = Concatenate()([x0, x])\n",
        "\n",
        "    return x0\n",
        "\n",
        "def se_block(input_tensor, num_filters, ratio=16):\n",
        "    \"\"\"Squeeze-and-Excitation Block.\"\"\"\n",
        "    se_shape = (1, 1, num_filters)\n",
        "    se = GlobalAveragePooling2D()(input_tensor)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(num_filters // ratio, activation='relu', use_bias=False, kernel_regularizer=l2(1e-4))(se)\n",
        "    se = Dense(num_filters, activation='sigmoid', use_bias=False, kernel_regularizer=l2(1e-4))(se)\n",
        "    return Multiply()([input_tensor, se])\n",
        "\n",
        "def scse_block(input_tensor, num_filters, ratio=16):\n",
        "    \"\"\"Concurrent Spatial and Channel Squeeze & Excitation (scSE) Block.\"\"\"\n",
        "    # Channel Squeeze & Excitation\n",
        "    cse = GlobalAveragePooling2D()(input_tensor)\n",
        "    cse = Reshape((1, 1, num_filters))(cse)\n",
        "    cse = Dense(num_filters // ratio, activation='relu', use_bias=False, kernel_regularizer=l2(1e-4))(cse)\n",
        "    cse = Dense(num_filters, activation='sigmoid', use_bias=False, kernel_regularizer=l2(1e-4))(cse)\n",
        "    channel_attention = Multiply()([input_tensor, cse])\n",
        "\n",
        "    # Spatial Squeeze & Excitation\n",
        "    sse = Conv2D(1, (1, 1), activation='sigmoid', use_bias=False, kernel_regularizer=l2(1e-4))(input_tensor)\n",
        "    spatial_attention = Multiply()([input_tensor, sse])\n",
        "\n",
        "    # Concurrent Fusion\n",
        "    scse_output = Add()([channel_attention, spatial_attention])\n",
        "    return scse_output\n",
        "\n",
        "def eca_block(input_tensor, b=1, gamma=2):\n",
        "    \"\"\"Efficient Channel Attention (ECA) Block.\"\"\"\n",
        "    num_channels = input_tensor.shape[-1]\n",
        "    kernel_size = int(abs((tf.math.log(float(num_channels), 2) + b) / gamma))\n",
        "    kernel_size = kernel_size if kernel_size % 2 == 1 else kernel_size + 1\n",
        "\n",
        "    x = GlobalAveragePooling2D()(input_tensor)\n",
        "    x = Reshape((1, 1, num_channels))(x)\n",
        "    x = Conv2D(1, kernel_size=kernel_size, padding='same', use_bias=False, groups=1,\n",
        "               kernel_regularizer=l2(1e-4))(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    return Multiply()([input_tensor, x])\n",
        "\n",
        "## Model\n",
        "\n",
        "def DeepLabV3PlusDenseDDSSPP(input_shape=(512, 512, 3), backbone_network='EfficientNetV2S', attention_module='se_block'):\n",
        "    \"\"\"DeepLabV3+ with DenseDDSSPP integration, accepting backbone and attention mechanism as parameters.\"\"\"\n",
        "    inputs = tf.keras.Input(input_shape)\n",
        "\n",
        "    if backbone_network == 'Xception':\n",
        "        base_model = Xception(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "        image_features_layer_name = 'block13_sepconv2_bn' # A deeper layer\n",
        "        skip_connection_layer_name = 'block3_sepconv2_bn' # A shallower layer\n",
        "    elif backbone_network == 'MobileNetV3Small':\n",
        "        base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "        image_features_layer_name = 'expanded_conv_7/depthwise/BatchNorm' # A deeper layer\n",
        "        skip_connection_layer_name = 'expanded_conv/depthwise_bn' # A shallower layer\n",
        "    elif backbone_network == 'EfficientNetV2S':\n",
        "        base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "        image_features_layer_name = 'block6a_expand_bn' # A deeper layer\n",
        "        skip_connection_layer_name = 'block2d_add' # A shallower layer\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported backbone network: {backbone_network}. Choose from 'Xception', 'MobileNetV3Small', 'EfficientNetV2S'.\")\n",
        "\n",
        "    image_features = base_model.get_layer(image_features_layer_name).output\n",
        "    spatial_shape_bfr_ddsspp = K.int_shape(image_features)[1:3]\n",
        "    print(f\"Ukuran spasial sebelum DenseDDSSPP: {spatial_shape_bfr_ddsspp}\")\n",
        "\n",
        "    x_a = DenseDDSSPP(image_features)\n",
        "    spatial_shape_after_ddsspp = K.int_shape(x_a)[1:3]\n",
        "    print(f\"Ukuran spasial setelah DenseDDSSPP: {spatial_shape_after_ddsspp}\")\n",
        "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
        "    spatial_shape_after_upsampling = K.int_shape(x_a)[1:3]\n",
        "    print(f\"Ukuran spasial setelah UpSampling2D: {spatial_shape_after_upsampling}\")\n",
        "\n",
        "    x_b = base_model.get_layer(skip_connection_layer_name).output\n",
        "    x_b = Conv2D(filters=48, kernel_size=1, padding='same', use_bias=False)(x_b)\n",
        "    x_b = BatchNormalization()(x_b)\n",
        "    x_b = Activation('relu')(x_b)\n",
        "\n",
        "    # Ensure x_b is resized to match x_a's spatial dimensions after upsampling\n",
        "    target_height, target_width = spatial_shape_after_upsampling\n",
        "    x_b = tf.image.resize(x_b, [target_height, target_width])\n",
        "\n",
        "    x = Concatenate()([x_a, x_b])\n",
        "    num_channels_after_concat = x.shape[-1]\n",
        "    print(f\"num_channels_after_concat: {num_channels_after_concat}\")\n",
        "\n",
        "    if attention_module == 'se_block':\n",
        "        x = se_block(x, num_filters=num_channels_after_concat)\n",
        "    elif attention_module == 'scse_block':\n",
        "        x = scse_block(x, num_filters=num_channels_after_concat)\n",
        "    elif attention_module == 'eca_block':\n",
        "        x = eca_block(x) # ECA block typically infers num_channels\n",
        "    else:\n",
        "        print(f\"Warning: Unknown attention module '{attention_module}'. No attention block applied.\")\n",
        "\n",
        "    for _ in range(4):  # Repeating Conv2D-BatchNorm-Activation layers\n",
        "        x = Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
        "    x = Conv2D(1, (1, 1), name='output_layer')(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "## Metrics\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    true_positives = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
        "    predicted_positives = tf.reduce_sum(tf.cast(y_pred, tf.float32))\n",
        "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    true_positives = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
        "    possible_positives = tf.reduce_sum(tf.cast(y_true, tf.float32))\n",
        "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "    return recall\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "def iou_m(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    intersection = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
        "    union = tf.reduce_sum(tf.cast(y_true + y_pred, tf.float32)) - intersection\n",
        "    iou = intersection / (union + tf.keras.backend.epsilon())\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Training\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/data/images\"\n",
        "MASK_DIR = \"/content/drive/MyDrive/data/masks\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/model_checkpoint.h5\"\n",
        "\n",
        "# Load and split dataset\n",
        "image_dataset, mask_dataset = load_dataset(IMAGE_DIR, MASK_DIR, size=512, max_images=4216)\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(image_dataset, mask_dataset)\n",
        "\n",
        "# Define and compile the model\n",
        "# Example usage:\n",
        "model = DeepLabV3PlusDenseDDSSPP(input_shape=(512, 512, 3), backbone_network='Xception', attention_module='scse_block')\n",
        "# Uncomment below to try other configurations\n",
        "# model = DeepLabV3PlusDenseDDSSPP(input_shape=(512, 512, 3), backbone_network='MobileNetV3Small', attention_module='eca_block')\n",
        "# model = DeepLabV3PlusDenseDDSSPP(input_shape=(512, 512, 3), backbone_network='EfficientNetV2S', attention_module='se_block')\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', iou_m, f1_m]\n",
        ")\n",
        "\n",
        "# Setup checkpointing\n",
        "checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=5,\n",
        "    callbacks=[checkpoint],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "Y-DS6sMQJ17c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing\n",
        "\n",
        "MODEL_PATH_BEST = \"/content/drive/MyDrive/model_checkpoint_512_Xception_seblock_best.h5\" # Using forward slashes\n",
        "\n",
        "# Free up memory\n",
        "del image_dataset, mask_dataset\n",
        "gc.collect()\n",
        "\n",
        "# Load the best model for testing\n",
        "# Rebuild the model structure before loading weights\n",
        "model = DeepLabV3PlusDenseDDSSPP(input_shape=(512, 512, 3), backbone_network='Xception', attention_module='scse_block') # Specify correct backbone and attention\n",
        "model.load_weights(MODEL_PATH_BEST)\n",
        "\n",
        "# Compile model for evaluation (optimizer doesn't affect evaluation, but metrics are needed)\n",
        "model.compile(\n",
        "    optimizer='adam', # Optimizer is required, but its value doesn't impact evaluation results\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[precision_m, recall_m, f1_m, iou_m]\n",
        ")\n",
        "\n",
        "# Set smaller batch size to avoid memory issues\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Evaluate the model on the test set with batching\n",
        "test_loss, test_precision, test_recall, test_f1, test_iou = model.evaluate(\n",
        "    X_test, y_test,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss}\")\n",
        "print(f\"Test Precision: {test_precision}\")\n",
        "print(f\"Test Recall: {test_recall}\")\n",
        "print(f\"Test F1 Score: {test_f1}\")\n",
        "print(f\"Test IoU: {test_iou}\")\n",
        "\n",
        "# Store results in a DataFrame and save to Excel\n",
        "results = pd.DataFrame({\n",
        "    'Metric': ['test_loss', 'test_precision', 'test_recall', 'test_f1', 'test_iou'],\n",
        "    'Value': [test_loss, test_precision, test_recall, test_f1, test_iou]\n",
        "})\n",
        "\n",
        "excel_file = 'test_results_512_Xception_seblock.xlsx'\n",
        "results.to_excel(excel_file, index=False)\n",
        "\n",
        "print(f\"\\nHasil testing disimpan dalam file: {excel_file}\")"
      ],
      "metadata": {
        "id": "PjJ5v8jNHe39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}